"""
Pentest Pipeline Orchestrator - Shannon-Inspired Architecture

Orchestrates the penetration testing workflow:
1. Pre-Reconnaissance (external scans)
2. Reconnaissance (code + browser analysis)  
3. Vulnerability Analysis (5 parallel agents)
4. Exploitation (proof-of-concept execution)
5. Reporting (professional pentest report)
"""

import asyncio
import json
import logging
import os
import subprocess
import time
from dataclasses import dataclass, field
from datetime import datetime, timezone
from enum import Enum
from pathlib import Path
from typing import Any, Callable, Optional

logger = logging.getLogger("artemis.pentest.pipeline")


class PentestPhase(str, Enum):
    """Pentest pipeline phases."""
    IDLE = "idle"
    PRE_RECON = "pre_recon"
    RECON = "recon"
    VULN_ANALYSIS = "vuln_analysis"
    EXPLOITATION = "exploitation"
    REPORTING = "reporting"
    COMPLETE = "complete"
    FAILED = "failed"


class VulnType(str, Enum):
    """Vulnerability types for parallel analysis."""
    INJECTION = "injection"  # SQL injection, command injection, LFI, SSTI
    XSS = "xss"  # Cross-site scripting
    AUTH = "auth"  # Authentication bypass, weak auth
    AUTHZ = "authz"  # Authorization flaws, privilege escalation
    SSRF = "ssrf"  # Server-side request forgery


@dataclass
class PentestConfig:
    """Configuration for a pentest run."""
    target_url: str
    repo_path: Optional[str] = None  # Path to source code (whitebox)
    
    # Authentication
    login_url: Optional[str] = None
    credentials: Optional[dict] = None  # {"username": ..., "password": ...}
    login_flow: Optional[list[str]] = None  # Step-by-step login instructions
    totp_secret: Optional[str] = None  # For 2FA
    
    # Scope
    avoid_paths: list[str] = field(default_factory=list)  # Paths to avoid
    focus_paths: list[str] = field(default_factory=list)  # Paths to focus on
    
    # LLM Settings
    provider: str = "ollama"
    model: str = "deepseek-r1:70b"
    
    # Output
    output_dir: Optional[str] = None
    workspace_name: Optional[str] = None
    
    # Features
    enable_browser: bool = True  # Use Playwright for exploitation
    enable_tools: bool = True  # Use nmap, subfinder, etc.
    parallel_agents: bool = True  # Run vuln analysis in parallel
    
    def __post_init__(self):
        if not self.output_dir:
            self.output_dir = f"./audit-logs/{self.workspace_name or 'pentest'}"
        if not self.workspace_name:
            hostname = self.target_url.replace("https://", "").replace("http://", "").split("/")[0]
            self.workspace_name = f"{hostname}_{int(time.time())}"


@dataclass
class AgentMetrics:
    """Metrics for a single agent run."""
    agent_name: str
    duration_ms: int = 0
    turns: int = 0
    tokens_used: int = 0
    cost_usd: float = 0.0
    success: bool = False
    error: Optional[str] = None
    findings_count: int = 0


@dataclass
class PipelineState:
    """Current state of the pentest pipeline."""
    status: PentestPhase = PentestPhase.IDLE
    current_agent: Optional[str] = None
    completed_agents: list[str] = field(default_factory=list)
    failed_agent: Optional[str] = None
    error: Optional[str] = None
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    agent_metrics: dict[str, AgentMetrics] = field(default_factory=dict)
    
    # Findings
    vulnerabilities: list[dict] = field(default_factory=list)
    recon_data: Optional[dict] = None
    
    # Progress tracking
    progress_percent: float = 0.0
    current_task: str = ""
    
    def to_dict(self) -> dict:
        return {
            "status": self.status.value,
            "current_agent": self.current_agent,
            "completed_agents": self.completed_agents,
            "failed_agent": self.failed_agent,
            "error": self.error,
            "start_time": self.start_time.isoformat() if self.start_time else None,
            "end_time": self.end_time.isoformat() if self.end_time else None,
            "progress_percent": self.progress_percent,
            "current_task": self.current_task,
            "vulnerability_count": len(self.vulnerabilities),
            "agent_metrics": {
                name: {
                    "duration_ms": m.duration_ms,
                    "success": m.success,
                    "findings_count": m.findings_count,
                }
                for name, m in self.agent_metrics.items()
            },
        }


class PentestPipeline:
    """
    Orchestrates the full pentest pipeline.
    
    Phases:
    1. Pre-Recon: External scans (nmap, subfinder, whatweb)
    2. Recon: Code analysis + browser exploration
    3. Vuln Analysis: 5 parallel agents for different vuln types
    4. Exploitation: Execute exploits to prove vulnerabilities
    5. Reporting: Generate professional pentest report
    """
    
    def __init__(
        self,
        config: PentestConfig,
        on_progress: Optional[Callable[[PipelineState], None]] = None,
    ):
        self.config = config
        self.state = PipelineState()
        self._on_progress = on_progress
        self._running = False
        self._cancelled = False
        
        # Create output directory
        self.output_dir = Path(config.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Deliverables directory
        self.deliverables_dir = self.output_dir / "deliverables"
        self.deliverables_dir.mkdir(exist_ok=True)
        
        # Agent logs directory
        self.agents_dir = self.output_dir / "agents"
        self.agents_dir.mkdir(exist_ok=True)
        
        # Initialize LLM client
        self._init_llm()
    
    def _init_llm(self):
        """Initialize LLM client for agent execution."""
        from ..llm import get_llm_client
        self.llm = get_llm_client(
            provider=self.config.provider,
            model=self.config.model,
        )
    
    def _emit_progress(self):
        """Emit progress update."""
        if self._on_progress:
            try:
                self._on_progress(self.state)
            except Exception as e:
                logger.error(f"Progress callback error: {e}")
    
    def _update_progress(self, percent: float, task: str):
        """Update progress state."""
        self.state.progress_percent = percent
        self.state.current_task = task
        self._emit_progress()
    
    async def run(self) -> PipelineState:
        """
        Execute the full pentest pipeline.
        
        Returns:
            PipelineState with all results and findings
        """
        if self._running:
            raise RuntimeError("Pipeline already running")
        
        self._running = True
        self._cancelled = False
        self.state.start_time = datetime.now(timezone.utc)
        self.state.status = PentestPhase.PRE_RECON
        
        logger.info("=" * 60)
        logger.info("ARTEMIS PENTEST PIPELINE STARTING")
        logger.info(f"Target: {self.config.target_url}")
        logger.info(f"Repo: {self.config.repo_path or 'N/A (blackbox)'}")
        logger.info(f"Model: {self.config.provider}/{self.config.model}")
        logger.info("=" * 60)
        
        try:
            # Phase 1: Pre-Reconnaissance
            await self._run_pre_recon()
            if self._cancelled:
                return self._finalize("cancelled")
            
            # Phase 2: Reconnaissance
            await self._run_recon()
            if self._cancelled:
                return self._finalize("cancelled")
            
            # Phase 3-4: Vulnerability Analysis + Exploitation
            await self._run_vuln_exploitation()
            if self._cancelled:
                return self._finalize("cancelled")
            
            # Phase 5: Reporting
            await self._run_reporting()
            
            return self._finalize("completed")
            
        except Exception as e:
            logger.error(f"Pipeline failed: {e}")
            self.state.error = str(e)
            return self._finalize("failed")
        
        finally:
            self._running = False
    
    def _finalize(self, status: str) -> PipelineState:
        """Finalize pipeline state."""
        self.state.end_time = datetime.now(timezone.utc)
        self.state.status = PentestPhase.COMPLETE if status == "completed" else PentestPhase.FAILED
        self._update_progress(100.0 if status == "completed" else self.state.progress_percent, f"Pipeline {status}")
        
        # Save session info
        self._save_session()
        
        logger.info("=" * 60)
        logger.info(f"PIPELINE {status.upper()}")
        logger.info(f"Duration: {(self.state.end_time - self.state.start_time).total_seconds():.1f}s")
        logger.info(f"Vulnerabilities found: {len(self.state.vulnerabilities)}")
        logger.info("=" * 60)
        
        return self.state
    
    def _save_session(self):
        """Save session metadata."""
        session_file = self.output_dir / "session.json"
        session_data = {
            "config": {
                "target_url": self.config.target_url,
                "repo_path": self.config.repo_path,
                "model": f"{self.config.provider}/{self.config.model}",
                "workspace": self.config.workspace_name,
            },
            "state": self.state.to_dict(),
            "summary": {
                "total_vulnerabilities": len(self.state.vulnerabilities),
                "by_severity": self._count_by_severity(),
                "by_type": self._count_by_type(),
            },
        }
        session_file.write_text(json.dumps(session_data, indent=2, default=str))
    
    def _count_by_severity(self) -> dict:
        """Count vulnerabilities by severity."""
        counts = {"critical": 0, "high": 0, "medium": 0, "low": 0, "info": 0}
        for v in self.state.vulnerabilities:
            sev = v.get("severity", "medium").lower()
            if sev in counts:
                counts[sev] += 1
        return counts
    
    def _count_by_type(self) -> dict:
        """Count vulnerabilities by type."""
        counts = {}
        for v in self.state.vulnerabilities:
            vtype = v.get("type", "unknown")
            counts[vtype] = counts.get(vtype, 0) + 1
        return counts
    
    def cancel(self):
        """Cancel the running pipeline."""
        self._cancelled = True
        logger.info("Pipeline cancellation requested")
    
    # =========================================================================
    # PHASE 1: PRE-RECONNAISSANCE
    # =========================================================================
    
    async def _run_pre_recon(self):
        """Run pre-reconnaissance phase with external tools."""
        self.state.status = PentestPhase.PRE_RECON
        self.state.current_agent = "pre-recon"
        self._update_progress(5.0, "Running external scans...")
        
        logger.info("Phase 1: Pre-Reconnaissance")
        start = time.time()
        
        pre_recon_data = {
            "target_url": self.config.target_url,
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "scans": {},
        }
        
        # Run nmap if available
        if self.config.enable_tools:
            nmap_result = await self._run_nmap()
            if nmap_result:
                pre_recon_data["scans"]["nmap"] = nmap_result
            
            # Run whatweb if available
            whatweb_result = await self._run_whatweb()
            if whatweb_result:
                pre_recon_data["scans"]["whatweb"] = whatweb_result
        
        # Analyze source code structure if available
        if self.config.repo_path:
            code_structure = await self._analyze_code_structure()
            pre_recon_data["code_structure"] = code_structure
        
        # Save pre-recon deliverable
        deliverable_path = self.deliverables_dir / "pre_recon_deliverable.md"
        self._write_pre_recon_deliverable(deliverable_path, pre_recon_data)
        
        duration_ms = int((time.time() - start) * 1000)
        self.state.agent_metrics["pre-recon"] = AgentMetrics(
            agent_name="pre-recon",
            duration_ms=duration_ms,
            success=True,
        )
        self.state.completed_agents.append("pre-recon")
        
        logger.info(f"Pre-recon complete in {duration_ms}ms")
    
    async def _run_nmap(self) -> Optional[dict]:
        """Run nmap scan on target."""
        try:
            from urllib.parse import urlparse
            parsed = urlparse(self.config.target_url)
            host = parsed.hostname
            
            # Quick service scan
            result = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: subprocess.run(
                    ["nmap", "-sV", "-T4", "--top-ports", "100", host],
                    capture_output=True,
                    text=True,
                    timeout=120,
                )
            )
            
            if result.returncode == 0:
                return {"output": result.stdout, "success": True}
            return {"error": result.stderr, "success": False}
            
        except FileNotFoundError:
            logger.debug("nmap not installed, skipping")
            return None
        except Exception as e:
            logger.warning(f"nmap failed: {e}")
            return {"error": str(e), "success": False}
    
    async def _run_whatweb(self) -> Optional[dict]:
        """Run whatweb for technology fingerprinting."""
        try:
            result = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: subprocess.run(
                    ["whatweb", "-a", "3", self.config.target_url],
                    capture_output=True,
                    text=True,
                    timeout=60,
                )
            )
            
            if result.returncode == 0:
                return {"output": result.stdout, "success": True}
            return {"error": result.stderr, "success": False}
            
        except FileNotFoundError:
            logger.debug("whatweb not installed, skipping")
            return None
        except Exception as e:
            logger.warning(f"whatweb failed: {e}")
            return {"error": str(e), "success": False}
    
    async def _analyze_code_structure(self) -> dict:
        """Analyze source code structure."""
        repo_path = Path(self.config.repo_path)
        if not repo_path.exists():
            return {"error": "Repository path not found"}
        
        structure = {
            "languages": {},
            "frameworks": [],
            "entry_points": [],
            "config_files": [],
        }
        
        # Count files by extension
        for ext in [".py", ".js", ".ts", ".go", ".java", ".php", ".rb"]:
            count = len(list(repo_path.rglob(f"*{ext}")))
            if count > 0:
                structure["languages"][ext] = count
        
        # Detect frameworks
        if (repo_path / "package.json").exists():
            structure["frameworks"].append("Node.js")
        if (repo_path / "requirements.txt").exists() or (repo_path / "pyproject.toml").exists():
            structure["frameworks"].append("Python")
        if (repo_path / "go.mod").exists():
            structure["frameworks"].append("Go")
        if (repo_path / "pom.xml").exists():
            structure["frameworks"].append("Java/Maven")
        if (repo_path / "composer.json").exists():
            structure["frameworks"].append("PHP/Composer")
        
        # Find config files
        for pattern in ["*.env*", "*config*", "*settings*"]:
            for f in repo_path.rglob(pattern):
                if f.is_file():
                    structure["config_files"].append(str(f.relative_to(repo_path)))
        
        return structure
    
    def _write_pre_recon_deliverable(self, path: Path, data: dict):
        """Write pre-recon deliverable markdown."""
        content = f"""# Pre-Reconnaissance Deliverable

## Target Information
- **URL:** {data['target_url']}
- **Timestamp:** {data['timestamp']}

## External Scans

### Nmap Results
```
{data.get('scans', {}).get('nmap', {}).get('output', 'Not available')}
```

### WhatWeb Results
```
{data.get('scans', {}).get('whatweb', {}).get('output', 'Not available')}
```

## Code Structure Analysis
```json
{json.dumps(data.get('code_structure', {}), indent=2)}
```
"""
        path.write_text(content)
    
    # =========================================================================
    # PHASE 2: RECONNAISSANCE
    # =========================================================================
    
    async def _run_recon(self):
        """Run reconnaissance phase with AI agent."""
        self.state.status = PentestPhase.RECON
        self.state.current_agent = "recon"
        self._update_progress(15.0, "Mapping attack surface...")
        
        logger.info("Phase 2: Reconnaissance")
        start = time.time()
        
        from .recon import ReconAgent
        
        agent = ReconAgent(
            config=self.config,
            llm=self.llm,
            deliverables_dir=self.deliverables_dir,
            agents_dir=self.agents_dir,
        )
        
        result = await agent.run()
        
        duration_ms = int((time.time() - start) * 1000)
        self.state.agent_metrics["recon"] = AgentMetrics(
            agent_name="recon",
            duration_ms=duration_ms,
            turns=result.turns,
            tokens_used=result.tokens_used,
            success=result.success,
            error=result.error,
        )
        self.state.completed_agents.append("recon")
        self.state.recon_data = result.data
        
        logger.info(f"Recon complete in {duration_ms}ms")
    
    # =========================================================================
    # PHASE 3-4: VULNERABILITY ANALYSIS + EXPLOITATION
    # =========================================================================
    
    async def _run_vuln_exploitation(self):
        """Run vulnerability analysis and exploitation phases."""
        self.state.status = PentestPhase.VULN_ANALYSIS
        self._update_progress(25.0, "Analyzing vulnerabilities...")
        
        logger.info("Phase 3-4: Vulnerability Analysis + Exploitation")
        
        from .vuln_agents import (
            InjectionVulnAgent,
            XSSVulnAgent,
            AuthVulnAgent,
            AuthzVulnAgent,
            SSRFVulnAgent,
        )
        from .exploit_agents import (
            InjectionExploitAgent,
            XSSExploitAgent,
            AuthExploitAgent,
            AuthzExploitAgent,
            SSRFExploitAgent,
        )
        
        # Define vuln/exploit pairs
        pipelines = [
            (VulnType.INJECTION, InjectionVulnAgent, InjectionExploitAgent),
            (VulnType.XSS, XSSVulnAgent, XSSExploitAgent),
            (VulnType.AUTH, AuthVulnAgent, AuthExploitAgent),
            (VulnType.AUTHZ, AuthzVulnAgent, AuthzExploitAgent),
            (VulnType.SSRF, SSRFVulnAgent, SSRFExploitAgent),
        ]
        
        if self.config.parallel_agents:
            # Run all pipelines in parallel
            tasks = [
                self._run_vuln_exploit_pipeline(vuln_type, vuln_cls, exploit_cls)
                for vuln_type, vuln_cls, exploit_cls in pipelines
            ]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in results:
                if isinstance(result, Exception):
                    logger.error(f"Pipeline failed: {result}")
        else:
            # Run sequentially
            for vuln_type, vuln_cls, exploit_cls in pipelines:
                try:
                    await self._run_vuln_exploit_pipeline(vuln_type, vuln_cls, exploit_cls)
                except Exception as e:
                    logger.error(f"{vuln_type} pipeline failed: {e}")
        
        self.state.status = PentestPhase.EXPLOITATION
        self._update_progress(75.0, "Exploitation complete")
    
    async def _run_vuln_exploit_pipeline(
        self,
        vuln_type: VulnType,
        vuln_agent_cls: type,
        exploit_agent_cls: type,
    ):
        """Run a single vuln analysis + exploitation pipeline."""
        vuln_agent_name = f"{vuln_type.value}-vuln"
        exploit_agent_name = f"{vuln_type.value}-exploit"
        
        logger.info(f"Running {vuln_type.value} pipeline")
        
        # Step 1: Vulnerability Analysis
        self.state.current_agent = vuln_agent_name
        start = time.time()
        
        vuln_agent = vuln_agent_cls(
            config=self.config,
            llm=self.llm,
            deliverables_dir=self.deliverables_dir,
            agents_dir=self.agents_dir,
            recon_data=self.state.recon_data,
        )
        
        vuln_result = await vuln_agent.run()
        
        duration_ms = int((time.time() - start) * 1000)
        self.state.agent_metrics[vuln_agent_name] = AgentMetrics(
            agent_name=vuln_agent_name,
            duration_ms=duration_ms,
            turns=vuln_result.turns,
            success=vuln_result.success,
            findings_count=len(vuln_result.findings),
        )
        self.state.completed_agents.append(vuln_agent_name)
        
        # Step 2: Check if exploitation needed
        if not vuln_result.findings:
            logger.info(f"No {vuln_type.value} vulnerabilities found, skipping exploitation")
            return
        
        # Step 3: Exploitation
        self.state.current_agent = exploit_agent_name
        start = time.time()
        
        exploit_agent = exploit_agent_cls(
            config=self.config,
            llm=self.llm,
            deliverables_dir=self.deliverables_dir,
            agents_dir=self.agents_dir,
            vulnerability_queue=vuln_result.findings,
        )
        
        exploit_result = await exploit_agent.run()
        
        duration_ms = int((time.time() - start) * 1000)
        self.state.agent_metrics[exploit_agent_name] = AgentMetrics(
            agent_name=exploit_agent_name,
            duration_ms=duration_ms,
            turns=exploit_result.turns,
            success=exploit_result.success,
            findings_count=len(exploit_result.findings),
        )
        self.state.completed_agents.append(exploit_agent_name)
        
        # Add confirmed vulnerabilities to state
        for finding in exploit_result.findings:
            if finding.get("exploited"):
                self.state.vulnerabilities.append(finding)
    
    # =========================================================================
    # PHASE 5: REPORTING
    # =========================================================================
    
    async def _run_reporting(self):
        """Generate final pentest report."""
        self.state.status = PentestPhase.REPORTING
        self.state.current_agent = "report"
        self._update_progress(85.0, "Generating report...")
        
        logger.info("Phase 5: Reporting")
        start = time.time()
        
        from .report import ReportGenerator
        
        generator = ReportGenerator(
            config=self.config,
            llm=self.llm,
            deliverables_dir=self.deliverables_dir,
            vulnerabilities=self.state.vulnerabilities,
            recon_data=self.state.recon_data,
            agent_metrics=self.state.agent_metrics,
        )
        
        result = await generator.generate()
        
        duration_ms = int((time.time() - start) * 1000)
        self.state.agent_metrics["report"] = AgentMetrics(
            agent_name="report",
            duration_ms=duration_ms,
            success=result.success,
        )
        self.state.completed_agents.append("report")
        
        logger.info(f"Report generated in {duration_ms}ms")
        logger.info(f"Report saved to: {result.report_path}")
