"""
Pentest Pipeline Orchestrator - Shannon-Inspired Architecture

Orchestrates the penetration testing workflow:
1. Pre-Reconnaissance (external scans)
2. Reconnaissance (code + browser analysis)  
3. Vulnerability Analysis (5 parallel agents)
4. Exploitation (proof-of-concept execution)
5. Reporting (professional pentest report)
"""

import asyncio
import json
import logging
import os
import subprocess
import time
from dataclasses import dataclass, field
from datetime import datetime, timezone
from enum import Enum
from pathlib import Path
from typing import Any, Callable, Optional

logger = logging.getLogger("artemis.pentest.pipeline")


class PentestPhase(str, Enum):
    """Pentest pipeline phases."""
    IDLE = "idle"
    PRE_RECON = "pre_recon"
    RECON = "recon"
    VULN_ANALYSIS = "vuln_analysis"
    EXPLOITATION = "exploitation"
    REPORTING = "reporting"
    COMPLETE = "complete"
    FAILED = "failed"


class VulnType(str, Enum):
    """Vulnerability types for parallel analysis."""
    INJECTION = "injection"  # SQL injection, command injection, LFI, SSTI
    XSS = "xss"  # Cross-site scripting
    AUTH = "auth"  # Authentication bypass, weak auth
    AUTHZ = "authz"  # Authorization flaws, privilege escalation
    SSRF = "ssrf"  # Server-side request forgery


@dataclass
class PentestConfig:
    """Configuration for a pentest run."""
    target_url: str
    repo_path: Optional[str] = None  # Path to source code (whitebox)
    
    # Authentication
    login_url: Optional[str] = None
    credentials: Optional[dict] = None  # {"username": ..., "password": ...}
    login_flow: Optional[list[str]] = None  # Step-by-step login instructions
    totp_secret: Optional[str] = None  # For 2FA
    
    # Scope
    avoid_paths: list[str] = field(default_factory=list)  # Paths to avoid
    focus_paths: list[str] = field(default_factory=list)  # Paths to focus on
    
    # LLM Settings
    provider: str = "ollama"
    model: str = "deepseek-r1:70b"
    
    # Output
    output_dir: Optional[str] = None
    workspace_name: Optional[str] = None
    
    # Features
    enable_browser: bool = True  # Use Playwright for exploitation
    enable_tools: bool = True  # Use nmap, subfinder, etc.
    parallel_agents: bool = True  # Run vuln analysis in parallel
    
    def __post_init__(self):
        if not self.output_dir:
            self.output_dir = f"./audit-logs/{self.workspace_name or 'pentest'}"
        if not self.workspace_name:
            hostname = self.target_url.replace("https://", "").replace("http://", "").split("/")[0]
            self.workspace_name = f"{hostname}_{int(time.time())}"


@dataclass
class AgentMetrics:
    """Metrics for a single agent run."""
    agent_name: str
    duration_ms: int = 0
    turns: int = 0
    tokens_used: int = 0
    cost_usd: float = 0.0
    success: bool = False
    error: Optional[str] = None
    findings_count: int = 0


@dataclass
class PipelineState:
    """Current state of the pentest pipeline."""
    status: PentestPhase = PentestPhase.IDLE
    current_agent: Optional[str] = None
    completed_agents: list[str] = field(default_factory=list)
    failed_agent: Optional[str] = None
    error: Optional[str] = None
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    agent_metrics: dict[str, AgentMetrics] = field(default_factory=dict)
    
    # Findings
    vulnerabilities: list[dict] = field(default_factory=list)
    recon_data: Optional[dict] = None
    
    # Progress tracking
    progress_percent: float = 0.0
    current_task: str = ""
    
    def to_dict(self) -> dict:
        return {
            "status": self.status.value,
            "current_agent": self.current_agent,
            "completed_agents": self.completed_agents,
            "failed_agent": self.failed_agent,
            "error": self.error,
            "start_time": self.start_time.isoformat() if self.start_time else None,
            "end_time": self.end_time.isoformat() if self.end_time else None,
            "progress_percent": self.progress_percent,
            "current_task": self.current_task,
            "vulnerability_count": len(self.vulnerabilities),
            "agent_metrics": {
                name: {
                    "duration_ms": m.duration_ms,
                    "success": m.success,
                    "findings_count": m.findings_count,
                }
                for name, m in self.agent_metrics.items()
            },
        }


class PentestPipeline:
    """
    Orchestrates the full pentest pipeline.
    
    Phases:
    1. Pre-Recon: External scans (nmap, subfinder, whatweb)
    2. Recon: Code analysis + browser exploration
    3. Vuln Analysis: 5 parallel agents for different vuln types
    4. Exploitation: Execute exploits to prove vulnerabilities
    5. Reporting: Generate professional pentest report
    """
    
    def __init__(
        self,
        config: PentestConfig,
        on_progress: Optional[Callable[[PipelineState], None]] = None,
        on_log: Optional[Callable[[str, str], None]] = None,
    ):
        self.config = config
        self.state = PipelineState()
        self._on_progress = on_progress
        self._on_log = on_log
        self._running = False
        self._cancelled = False
        
        # Create output directory
        self.output_dir = Path(config.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Deliverables directory
        self.deliverables_dir = self.output_dir / "deliverables"
        self.deliverables_dir.mkdir(exist_ok=True)
        
        # Agent logs directory
        self.agents_dir = self.output_dir / "agents"
        self.agents_dir.mkdir(exist_ok=True)
        
        # Initialize LLM client
        self._init_llm()
    
    def _init_llm(self):
        """Initialize LLM client for agent execution."""
        from ..llm import get_llm_client
        self.llm = get_llm_client(
            provider=self.config.provider,
            model=self.config.model,
        )
    
    def _emit_progress(self):
        """Emit progress update."""
        if self._on_progress:
            try:
                self._on_progress(self.state)
            except Exception as e:
                logger.error(f"Progress callback error: {e}")
    
    def _log(self, level: str, message: str):
        """Emit a log entry."""
        logger.log(
            logging.DEBUG if level == "debug" else
            logging.WARNING if level == "warn" else
            logging.ERROR if level == "error" else
            logging.INFO,
            message
        )
        if self._on_log:
            try:
                self._on_log(level, message)
            except Exception as e:
                logger.error(f"Log callback error: {e}")
    
    def _update_progress(self, percent: float, task: str):
        """Update progress state."""
        self.state.progress_percent = percent
        self.state.current_task = task
        self._emit_progress()
    
    async def run(self) -> PipelineState:
        """
        Execute the full pentest pipeline.
        
        Returns:
            PipelineState with all results and findings
        """
        if self._running:
            raise RuntimeError("Pipeline already running")
        
        self._running = True
        self._cancelled = False
        self.state.start_time = datetime.now(timezone.utc)
        self.state.status = PentestPhase.PRE_RECON
        
        self._log("info", "=" * 50)
        self._log("info", "ARTEMIS PENTEST PIPELINE STARTING")
        self._log("info", "=" * 50)
        self._log("info", f"Target: {self.config.target_url}")
        self._log("info", f"Repo: {self.config.repo_path or 'N/A (blackbox)'}")
        self._log("info", f"Model: {self.config.provider}/{self.config.model}")
        self._log("info", "-" * 50)
        
        try:
            # Phase 1: Pre-Reconnaissance
            self._log("phase", "PHASE 1: Pre-Reconnaissance")
            await self._run_pre_recon()
            if self._cancelled:
                self._log("warn", "Pipeline cancelled by user")
                return self._finalize("cancelled")
            
            # Phase 2: Reconnaissance
            self._log("phase", "PHASE 2: Reconnaissance")
            await self._run_recon()
            if self._cancelled:
                self._log("warn", "Pipeline cancelled by user")
                return self._finalize("cancelled")
            
            # Phase 3-4: Vulnerability Analysis + Exploitation
            self._log("phase", "PHASE 3-4: Vulnerability Analysis + Exploitation")
            await self._run_vuln_exploitation()
            if self._cancelled:
                self._log("warn", "Pipeline cancelled by user")
                return self._finalize("cancelled")
            
            # Phase 5: Reporting
            self._log("phase", "PHASE 5: Report Generation")
            await self._run_reporting()
            
            self._log("success", "Pipeline completed successfully")
            return self._finalize("completed")
            
        except Exception as e:
            self._log("error", f"Pipeline failed: {e}")
            logger.error(f"Pipeline failed: {e}")
            self.state.error = str(e)
            return self._finalize("failed")
        
        finally:
            self._running = False
    
    def _finalize(self, status: str) -> PipelineState:
        """Finalize pipeline state."""
        self.state.end_time = datetime.now(timezone.utc)
        self.state.status = PentestPhase.COMPLETE if status == "completed" else PentestPhase.FAILED
        self._update_progress(100.0 if status == "completed" else self.state.progress_percent, f"Pipeline {status}")
        
        # Save session info
        self._save_session()
        
        logger.info("=" * 60)
        logger.info(f"PIPELINE {status.upper()}")
        logger.info(f"Duration: {(self.state.end_time - self.state.start_time).total_seconds():.1f}s")
        logger.info(f"Vulnerabilities found: {len(self.state.vulnerabilities)}")
        logger.info("=" * 60)
        
        return self.state
    
    def _save_session(self):
        """Save session metadata."""
        session_file = self.output_dir / "session.json"
        session_data = {
            "config": {
                "target_url": self.config.target_url,
                "repo_path": self.config.repo_path,
                "model": f"{self.config.provider}/{self.config.model}",
                "workspace": self.config.workspace_name,
            },
            "state": self.state.to_dict(),
            "summary": {
                "total_vulnerabilities": len(self.state.vulnerabilities),
                "by_severity": self._count_by_severity(),
                "by_type": self._count_by_type(),
            },
        }
        session_file.write_text(json.dumps(session_data, indent=2, default=str))
    
    def _count_by_severity(self) -> dict:
        """Count vulnerabilities by severity."""
        counts = {"critical": 0, "high": 0, "medium": 0, "low": 0, "info": 0}
        for v in self.state.vulnerabilities:
            sev = v.get("severity", "medium").lower()
            if sev in counts:
                counts[sev] += 1
        return counts
    
    def _count_by_type(self) -> dict:
        """Count vulnerabilities by type."""
        counts = {}
        for v in self.state.vulnerabilities:
            vtype = v.get("type", "unknown")
            counts[vtype] = counts.get(vtype, 0) + 1
        return counts
    
    def cancel(self):
        """Cancel the running pipeline."""
        self._cancelled = True
        logger.info("Pipeline cancellation requested")
    
    # =========================================================================
    # PHASE 1: PRE-RECONNAISSANCE
    # =========================================================================
    
    async def _run_pre_recon(self):
        """Run pre-reconnaissance phase with external tools."""
        self.state.status = PentestPhase.PRE_RECON
        self.state.current_agent = "pre-recon"
        self._update_progress(5.0, "Running external scans...")
        self._emit_progress()
        
        self._log("agent", "Starting pre-recon agent")
        start = time.time()
        
        pre_recon_data = {
            "target_url": self.config.target_url,
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "scans": {},
        }
        
        # Run security tools if available
        if self.config.enable_tools:
            from .tools import get_tools
            tools = get_tools()
            available = tools.get_available_tools()
            self._log("info", f"Available tools: {', '.join(available) if available else 'none'}")
            
            # Run nmap
            nmap_result = await self._run_nmap()
            if nmap_result:
                pre_recon_data["scans"]["nmap"] = nmap_result
            
            # Run httpx for HTTP probing and tech detection
            httpx_result = await self._run_httpx()
            if httpx_result:
                pre_recon_data["scans"]["httpx"] = httpx_result
            
            # Run subfinder for subdomain enumeration
            subfinder_result = await self._run_subfinder()
            if subfinder_result:
                pre_recon_data["scans"]["subfinder"] = subfinder_result
            
            # Run katana for web crawling
            katana_result = await self._run_katana()
            if katana_result:
                pre_recon_data["scans"]["katana"] = katana_result
        
        # Analyze source code structure if available
        if self.config.repo_path:
            self._log("info", f"Analyzing source code: {self.config.repo_path}")
            code_structure = await self._analyze_code_structure()
            pre_recon_data["code_structure"] = code_structure
            langs = list(code_structure.get("languages", {}).keys())
            self._log("info", f"Languages detected: {', '.join(langs) if langs else 'none'}")
        
        # Save pre-recon deliverable
        deliverable_path = self.deliverables_dir / "pre_recon_deliverable.md"
        self._write_pre_recon_deliverable(deliverable_path, pre_recon_data)
        
        duration_ms = int((time.time() - start) * 1000)
        self.state.agent_metrics["pre-recon"] = AgentMetrics(
            agent_name="pre-recon",
            duration_ms=duration_ms,
            success=True,
        )
        self.state.completed_agents.append("pre-recon")
        self._emit_progress()
        
        self._log("success", f"Pre-recon complete in {duration_ms}ms")
    
    async def _run_nmap(self) -> Optional[dict]:
        """Run nmap scan on target."""
        from .tools import get_tools
        from urllib.parse import urlparse
        
        tools = get_tools()
        if not tools.is_available("nmap"):
            self._log("warn", "nmap not available, skipping")
            return None
        
        parsed = urlparse(self.config.target_url)
        host = parsed.hostname
        
        self._log("http", f"Running nmap scan on {host}...")
        result = await tools.nmap_scan(host, ports="1-1000", service_detection=True)
        
        if result.success:
            # Log discovered services
            if result.parsed and result.parsed.get("hosts"):
                for host_data in result.parsed["hosts"]:
                    open_ports = [p for p in host_data.get("ports", []) if p.get("state") == "open"]
                    self._log("info", f"Found {len(open_ports)} open ports on {host}")
                    for port in open_ports[:10]:  # Log first 10
                        service = port.get("service", {})
                        svc_name = service.get("name", "unknown")
                        product = service.get("product", "")
                        self._log("debug", f"  Port {port['port']}: {svc_name} {product}")
            
            return {"output": result.output, "parsed": result.parsed, "success": True}
        else:
            self._log("warn", f"nmap failed: {result.error}")
            return {"error": result.error, "success": False}
    
    async def _run_httpx(self) -> Optional[dict]:
        """Run httpx for HTTP probing and tech detection."""
        from .tools import get_tools
        
        tools = get_tools()
        if not tools.is_available("httpx"):
            self._log("debug", "httpx not available, skipping")
            return None
        
        self._log("http", "Running httpx probe...")
        result = await tools.httpx_probe([self.config.target_url])
        
        if result.success and result.parsed:
            probes = result.parsed.get("probes", [])
            for probe in probes:
                techs = probe.get("technologies", [])
                if techs:
                    self._log("info", f"Technologies detected: {', '.join(techs[:5])}")
            return {"output": result.output, "parsed": result.parsed, "success": True}
        else:
            self._log("warn", f"httpx failed: {result.error}")
            return {"error": result.error, "success": False}
    
    async def _run_subfinder(self) -> Optional[dict]:
        """Run subfinder for subdomain enumeration."""
        from .tools import get_tools
        from urllib.parse import urlparse
        
        tools = get_tools()
        if not tools.is_available("subfinder"):
            self._log("debug", "subfinder not available, skipping")
            return None
        
        parsed = urlparse(self.config.target_url)
        domain = parsed.hostname
        
        # Only run on actual domains, not IPs
        if domain and not domain.replace(".", "").isdigit():
            self._log("http", f"Enumerating subdomains for {domain}...")
            result = await tools.subfinder_scan(domain)
            
            if result.success and result.parsed:
                subdomains = result.parsed.get("subdomains", [])
                self._log("info", f"Found {len(subdomains)} subdomains")
                return {"output": result.output, "parsed": result.parsed, "success": True}
        
        return None
    
    async def _run_katana(self) -> Optional[dict]:
        """Run katana for web crawling."""
        from .tools import get_tools
        
        tools = get_tools()
        if not tools.is_available("katana"):
            self._log("debug", "katana not available, skipping")
            return None
        
        self._log("http", "Crawling target with katana...")
        result = await tools.katana_crawl(self.config.target_url, depth=2)
        
        if result.success and result.parsed:
            endpoints = result.parsed.get("total_urls", 0)
            params = len(result.parsed.get("parameters", []))
            self._log("info", f"Crawled {endpoints} endpoints, found {params} parameters")
            return {"output": result.output, "parsed": result.parsed, "success": True}
        else:
            self._log("warn", f"katana failed: {result.error}")
            return {"error": result.error, "success": False}
    
    async def _analyze_code_structure(self) -> dict:
        """Analyze source code structure."""
        repo_path = Path(self.config.repo_path)
        if not repo_path.exists():
            return {"error": "Repository path not found"}
        
        structure = {
            "languages": {},
            "frameworks": [],
            "entry_points": [],
            "config_files": [],
        }
        
        # Count files by extension
        for ext in [".py", ".js", ".ts", ".go", ".java", ".php", ".rb"]:
            count = len(list(repo_path.rglob(f"*{ext}")))
            if count > 0:
                structure["languages"][ext] = count
        
        # Detect frameworks
        if (repo_path / "package.json").exists():
            structure["frameworks"].append("Node.js")
        if (repo_path / "requirements.txt").exists() or (repo_path / "pyproject.toml").exists():
            structure["frameworks"].append("Python")
        if (repo_path / "go.mod").exists():
            structure["frameworks"].append("Go")
        if (repo_path / "pom.xml").exists():
            structure["frameworks"].append("Java/Maven")
        if (repo_path / "composer.json").exists():
            structure["frameworks"].append("PHP/Composer")
        
        # Find config files
        for pattern in ["*.env*", "*config*", "*settings*"]:
            for f in repo_path.rglob(pattern):
                if f.is_file():
                    structure["config_files"].append(str(f.relative_to(repo_path)))
        
        return structure
    
    def _write_pre_recon_deliverable(self, path: Path, data: dict):
        """Write pre-recon deliverable markdown."""
        content = f"""# Pre-Reconnaissance Deliverable

## Target Information
- **URL:** {data['target_url']}
- **Timestamp:** {data['timestamp']}

## External Scans

### Nmap Results
```
{data.get('scans', {}).get('nmap', {}).get('output', 'Not available')}
```

### WhatWeb Results
```
{data.get('scans', {}).get('whatweb', {}).get('output', 'Not available')}
```

## Code Structure Analysis
```json
{json.dumps(data.get('code_structure', {}), indent=2)}
```
"""
        path.write_text(content)
    
    # =========================================================================
    # PHASE 2: RECONNAISSANCE
    # =========================================================================
    
    async def _run_recon(self):
        """Run reconnaissance phase with AI agent."""
        self.state.status = PentestPhase.RECON
        self.state.current_agent = "recon"
        self._update_progress(15.0, "Mapping attack surface...")
        self._emit_progress()
        
        self._log("agent", "Starting recon agent")
        self._log("llm", f"Querying {self.config.model} for attack surface analysis...")
        start = time.time()
        
        from .recon import ReconAgent
        
        agent = ReconAgent(
            config=self.config,
            llm=self.llm,
            deliverables_dir=self.deliverables_dir,
            agents_dir=self.agents_dir,
        )
        
        result = await agent.run()
        
        duration_ms = int((time.time() - start) * 1000)
        self.state.agent_metrics["recon"] = AgentMetrics(
            agent_name="recon",
            duration_ms=duration_ms,
            turns=result.turns,
            tokens_used=result.tokens_used,
            success=result.success,
            error=result.error,
        )
        self.state.completed_agents.append("recon")
        self.state.recon_data = result.data
        self._emit_progress()
        
        if result.success:
            endpoints = len(result.data.get("endpoints", [])) if result.data else 0
            self._log("success", f"Recon complete: {endpoints} endpoints found ({duration_ms}ms)")
        else:
            self._log("error", f"Recon failed: {result.error}")
    
    # =========================================================================
    # PHASE 3-4: VULNERABILITY ANALYSIS + EXPLOITATION
    # =========================================================================
    
    async def _run_vuln_exploitation(self):
        """Run vulnerability analysis and exploitation phases."""
        self.state.status = PentestPhase.VULN_ANALYSIS
        self._update_progress(25.0, "Analyzing vulnerabilities...")
        self._emit_progress()
        
        self._log("info", "Starting vulnerability analysis...")
        mode = "parallel" if self.config.parallel_agents else "sequential"
        self._log("info", f"Running 5 vuln agents in {mode} mode")
        
        from .vuln_agents import (
            InjectionVulnAgent,
            XSSVulnAgent,
            AuthVulnAgent,
            AuthzVulnAgent,
            SSRFVulnAgent,
        )
        from .exploit_agents import (
            InjectionExploitAgent,
            XSSExploitAgent,
            AuthExploitAgent,
            AuthzExploitAgent,
            SSRFExploitAgent,
        )
        
        # Define vuln/exploit pairs
        pipelines = [
            (VulnType.INJECTION, InjectionVulnAgent, InjectionExploitAgent),
            (VulnType.XSS, XSSVulnAgent, XSSExploitAgent),
            (VulnType.AUTH, AuthVulnAgent, AuthExploitAgent),
            (VulnType.AUTHZ, AuthzVulnAgent, AuthzExploitAgent),
            (VulnType.SSRF, SSRFVulnAgent, SSRFExploitAgent),
        ]
        
        if self.config.parallel_agents:
            # Run all pipelines in parallel
            self._log("info", "Launching parallel vuln analysis...")
            tasks = [
                self._run_vuln_exploit_pipeline(vuln_type, vuln_cls, exploit_cls)
                for vuln_type, vuln_cls, exploit_cls in pipelines
            ]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    self._log("error", f"Pipeline {pipelines[i][0].value} failed: {result}")
        else:
            # Run sequentially
            for vuln_type, vuln_cls, exploit_cls in pipelines:
                try:
                    await self._run_vuln_exploit_pipeline(vuln_type, vuln_cls, exploit_cls)
                except Exception as e:
                    self._log("error", f"{vuln_type} pipeline failed: {e}")
        
        self.state.status = PentestPhase.EXPLOITATION
        self._update_progress(75.0, "Exploitation complete")
        self._emit_progress()
        
        total_vulns = len(self.state.vulnerabilities)
        self._log("info", f"Vulnerability analysis complete: {total_vulns} confirmed vulnerabilities")
    
    async def _run_vuln_exploit_pipeline(
        self,
        vuln_type: VulnType,
        vuln_agent_cls: type,
        exploit_agent_cls: type,
    ):
        """Run a single vuln analysis + exploitation pipeline."""
        vuln_agent_name = f"{vuln_type.value}-vuln"
        exploit_agent_name = f"{vuln_type.value}-exploit"
        
        self._log("agent", f"[{vuln_type.value.upper()}] Starting vulnerability analysis")
        
        # Step 1: Vulnerability Analysis
        self.state.current_agent = vuln_agent_name
        self._emit_progress()
        start = time.time()
        
        self._log("llm", f"[{vuln_type.value.upper()}] Querying AI for {vuln_type.value} vulnerabilities...")
        
        vuln_agent = vuln_agent_cls(
            config=self.config,
            llm=self.llm,
            deliverables_dir=self.deliverables_dir,
            agents_dir=self.agents_dir,
            recon_data=self.state.recon_data,
        )
        
        vuln_result = await vuln_agent.run()
        
        duration_ms = int((time.time() - start) * 1000)
        self.state.agent_metrics[vuln_agent_name] = AgentMetrics(
            agent_name=vuln_agent_name,
            duration_ms=duration_ms,
            turns=vuln_result.turns,
            success=vuln_result.success,
            findings_count=len(vuln_result.findings),
        )
        self.state.completed_agents.append(vuln_agent_name)
        self._emit_progress()
        
        findings_count = len(vuln_result.findings)
        self._log("info", f"[{vuln_type.value.upper()}] Analysis complete: {findings_count} potential vulnerabilities ({duration_ms}ms)")
        
        # Step 2: Check if exploitation needed
        if not vuln_result.findings:
            self._log("debug", f"[{vuln_type.value.upper()}] No vulnerabilities to exploit, skipping")
            return
        
        # Step 3: Exploitation
        self._log("agent", f"[{vuln_type.value.upper()}] Starting exploitation of {findings_count} vulnerabilities")
        self.state.current_agent = exploit_agent_name
        self._emit_progress()
        start = time.time()
        
        exploit_agent = exploit_agent_cls(
            config=self.config,
            llm=self.llm,
            deliverables_dir=self.deliverables_dir,
            agents_dir=self.agents_dir,
            vulnerability_queue=vuln_result.findings,
        )
        
        exploit_result = await exploit_agent.run()
        
        duration_ms = int((time.time() - start) * 1000)
        self.state.agent_metrics[exploit_agent_name] = AgentMetrics(
            agent_name=exploit_agent_name,
            duration_ms=duration_ms,
            turns=exploit_result.turns,
            success=exploit_result.success,
            findings_count=len(exploit_result.findings),
        )
        self.state.completed_agents.append(exploit_agent_name)
        self._emit_progress()
        
        # Add confirmed vulnerabilities to state
        exploited_count = 0
        for finding in exploit_result.findings:
            if finding.get("exploited"):
                self.state.vulnerabilities.append(finding)
                exploited_count += 1
                self._log("vuln", f"[{vuln_type.value.upper()}] CONFIRMED: {finding.get('title', 'Unknown')}")
        
        self._log("info", f"[{vuln_type.value.upper()}] Exploitation complete: {exploited_count}/{findings_count} confirmed ({duration_ms}ms)")
    
    # =========================================================================
    # PHASE 5: REPORTING
    # =========================================================================
    
    async def _run_reporting(self):
        """Generate final pentest report."""
        self.state.status = PentestPhase.REPORTING
        self.state.current_agent = "report"
        self._update_progress(85.0, "Generating report...")
        self._emit_progress()
        
        self._log("agent", "Starting report generation")
        self._log("info", f"Compiling {len(self.state.vulnerabilities)} vulnerabilities into report...")
        start = time.time()
        
        from .report import ReportGenerator
        
        generator = ReportGenerator(
            config=self.config,
            llm=self.llm,
            deliverables_dir=self.deliverables_dir,
            vulnerabilities=self.state.vulnerabilities,
            recon_data=self.state.recon_data,
            agent_metrics=self.state.agent_metrics,
        )
        
        self._log("llm", "Generating executive summary with AI...")
        result = await generator.generate()
        
        duration_ms = int((time.time() - start) * 1000)
        self.state.agent_metrics["report"] = AgentMetrics(
            agent_name="report",
            duration_ms=duration_ms,
            success=result.success,
        )
        self.state.completed_agents.append("report")
        self._emit_progress()
        
        if result.success:
            self._log("success", f"Report generated in {duration_ms}ms")
            self._log("info", f"Report saved to: {result.deliverable_path}")
        else:
            self._log("error", f"Report generation failed: {result.error}")
