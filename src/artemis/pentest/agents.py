"""
Base Agent Classes for Pentest Pipeline

Provides the foundation for all specialized agents:
- ReconAgent
- VulnAgents (Injection, XSS, Auth, Authz, SSRF)
- ExploitAgents
- ReportAgent
"""

import asyncio
import json
import logging
import time
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Optional

logger = logging.getLogger("artemis.pentest.agents")


@dataclass
class AgentResult:
    """Result from an agent execution."""
    success: bool
    turns: int = 0
    tokens_used: int = 0
    duration_ms: int = 0
    findings: list[dict] = field(default_factory=list)
    data: Optional[dict] = None
    error: Optional[str] = None
    deliverable_path: Optional[str] = None


@dataclass 
class Finding:
    """A security finding/vulnerability."""
    id: str
    title: str
    type: str  # injection, xss, auth, authz, ssrf
    severity: str  # critical, high, medium, low, info
    
    # Location
    endpoint: Optional[str] = None
    method: Optional[str] = None
    parameter: Optional[str] = None
    file_path: Optional[str] = None
    line_number: Optional[int] = None
    
    # Details
    description: str = ""
    impact: str = ""
    remediation: str = ""
    
    # Evidence
    request: Optional[str] = None
    response: Optional[str] = None
    payload: Optional[str] = None
    screenshot: Optional[str] = None
    
    # Exploitation status
    exploited: bool = False
    exploit_proof: Optional[str] = None
    
    # Confidence
    confidence: float = 0.0  # 0-1
    false_positive: bool = False
    
    def to_dict(self) -> dict:
        return {
            "id": self.id,
            "title": self.title,
            "type": self.type,
            "severity": self.severity,
            "endpoint": self.endpoint,
            "method": self.method,
            "parameter": self.parameter,
            "file_path": self.file_path,
            "line_number": self.line_number,
            "description": self.description,
            "impact": self.impact,
            "remediation": self.remediation,
            "request": self.request,
            "response": self.response,
            "payload": self.payload,
            "exploited": self.exploited,
            "exploit_proof": self.exploit_proof,
            "confidence": self.confidence,
        }


class BaseAgent(ABC):
    """
    Base class for all pentest agents.
    
    Provides:
    - LLM integration
    - Logging and metrics
    - Deliverable management
    - Tool execution
    """
    
    agent_name: str = "base"
    agent_role: str = "Base Agent"
    
    def __init__(
        self,
        config,
        llm,
        deliverables_dir: Path,
        agents_dir: Path,
        **kwargs,
    ):
        self.config = config
        self.llm = llm
        self.deliverables_dir = Path(deliverables_dir)
        self.agents_dir = Path(agents_dir)
        
        # Agent-specific log directory
        self.log_dir = self.agents_dir / self.agent_name
        self.log_dir.mkdir(exist_ok=True)
        
        # Metrics
        self.turns = 0
        self.tokens_used = 0
        self.start_time: Optional[float] = None
        
        # Store any additional kwargs
        for key, value in kwargs.items():
            setattr(self, key, value)
    
    @abstractmethod
    async def run(self) -> AgentResult:
        """Execute the agent's task."""
        pass
    
    @abstractmethod
    def get_system_prompt(self) -> str:
        """Get the agent's system prompt."""
        pass
    
    async def query_llm(self, user_prompt: str, system_prompt: Optional[str] = None) -> str:
        """Query the LLM with the given prompt."""
        if system_prompt is None:
            system_prompt = self.get_system_prompt()
        
        self.turns += 1
        
        try:
            response = await self._call_llm(system_prompt, user_prompt)
            return response
        except Exception as e:
            logger.error(f"LLM query failed: {e}")
            raise
    
    async def _call_llm(self, system_prompt: str, user_prompt: str) -> str:
        """Internal LLM call with provider-specific handling."""
        import httpx
        
        if self.config.provider == "ollama":
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    "http://localhost:11434/api/generate",
                    json={
                        "model": self.config.model,
                        "system": system_prompt,
                        "prompt": user_prompt,
                        "stream": False,
                        "options": {
                            "temperature": 0.1,  # Low temp for analysis
                            "num_predict": 8192,
                        },
                    },
                    timeout=300.0,
                )
                
                if response.status_code == 200:
                    data = response.json()
                    self.tokens_used += data.get("eval_count", 0)
                    return data.get("response", "")
                else:
                    raise Exception(f"Ollama error: {response.status_code}")
        
        elif self.config.provider == "anthropic":
            # Anthropic Claude
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    "https://api.anthropic.com/v1/messages",
                    headers={
                        "x-api-key": self.config.api_key,
                        "anthropic-version": "2023-06-01",
                        "content-type": "application/json",
                    },
                    json={
                        "model": self.config.model,
                        "max_tokens": 8192,
                        "system": system_prompt,
                        "messages": [{"role": "user", "content": user_prompt}],
                    },
                    timeout=300.0,
                )
                
                if response.status_code == 200:
                    data = response.json()
                    self.tokens_used += data.get("usage", {}).get("output_tokens", 0)
                    return data["content"][0]["text"]
                else:
                    raise Exception(f"Anthropic error: {response.status_code}")
        
        elif self.config.provider == "openai":
            # OpenAI
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    "https://api.openai.com/v1/chat/completions",
                    headers={
                        "Authorization": f"Bearer {self.config.api_key}",
                        "Content-Type": "application/json",
                    },
                    json={
                        "model": self.config.model,
                        "messages": [
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt},
                        ],
                        "max_tokens": 8192,
                        "temperature": 0.1,
                    },
                    timeout=300.0,
                )
                
                if response.status_code == 200:
                    data = response.json()
                    self.tokens_used += data.get("usage", {}).get("completion_tokens", 0)
                    return data["choices"][0]["message"]["content"]
                else:
                    raise Exception(f"OpenAI error: {response.status_code}")
        
        else:
            raise ValueError(f"Unknown provider: {self.config.provider}")
    
    def read_deliverable(self, name: str) -> str:
        """Read a deliverable file."""
        path = self.deliverables_dir / name
        if path.exists():
            return path.read_text()
        return ""
    
    def write_deliverable(self, name: str, content: str):
        """Write a deliverable file."""
        path = self.deliverables_dir / name
        path.write_text(content)
        logger.info(f"Wrote deliverable: {path}")
    
    def save_findings(self, findings: list[Finding], queue_name: str):
        """Save findings to a JSON queue file."""
        path = self.deliverables_dir / f"{queue_name}.json"
        data = {
            "agent": self.agent_name,
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "vulnerabilities": [f.to_dict() for f in findings],
        }
        path.write_text(json.dumps(data, indent=2))
        logger.info(f"Saved {len(findings)} findings to {path}")
    
    def load_queue(self, queue_name: str) -> list[dict]:
        """Load a vulnerability queue."""
        path = self.deliverables_dir / f"{queue_name}.json"
        if path.exists():
            data = json.loads(path.read_text())
            return data.get("vulnerabilities", [])
        return []
    
    def log_turn(self, turn_num: int, action: str, result: str):
        """Log an agent turn."""
        log_file = self.log_dir / f"turn_{turn_num:03d}.md"
        content = f"""# Turn {turn_num}

## Action
{action}

## Result
{result}

## Timestamp
{datetime.now(timezone.utc).isoformat()}
"""
        log_file.write_text(content)
    
    async def run_tool(self, tool_name: str, **kwargs) -> dict:
        """Run an external tool."""
        # This can be extended with actual tool implementations
        # (nmap, browser automation, etc.)
        logger.info(f"Running tool: {tool_name} with {kwargs}")
        return {"success": True, "output": "Tool execution placeholder"}


class VulnAgent(BaseAgent):
    """Base class for vulnerability analysis agents."""
    
    vuln_type: str = "unknown"
    
    def __init__(self, recon_data: Optional[dict] = None, **kwargs):
        super().__init__(**kwargs)
        self.recon_data = recon_data or {}
    
    def get_recon_context(self) -> str:
        """Get relevant context from recon data."""
        recon_path = self.deliverables_dir / "recon_deliverable.md"
        if recon_path.exists():
            return recon_path.read_text()
        return json.dumps(self.recon_data, indent=2)
    
    async def run(self) -> AgentResult:
        """Run vulnerability analysis."""
        self.start_time = time.time()
        
        try:
            # Load recon context
            recon_context = self.get_recon_context()
            
            # Build analysis prompt
            prompt = self.build_analysis_prompt(recon_context)
            
            # Query LLM for analysis
            response = await self.query_llm(prompt)
            
            # Parse findings
            findings = self.parse_findings(response)
            
            # Save to queue
            queue_name = f"{self.vuln_type}_exploitation_queue"
            self.save_findings(findings, queue_name)
            
            # Write analysis deliverable
            deliverable_name = f"{self.vuln_type}_analysis.md"
            self.write_deliverable(deliverable_name, response)
            
            duration_ms = int((time.time() - self.start_time) * 1000)
            
            return AgentResult(
                success=True,
                turns=self.turns,
                tokens_used=self.tokens_used,
                duration_ms=duration_ms,
                findings=[f.to_dict() for f in findings],
                deliverable_path=str(self.deliverables_dir / deliverable_name),
            )
            
        except Exception as e:
            logger.error(f"{self.agent_name} failed: {e}")
            return AgentResult(
                success=False,
                turns=self.turns,
                error=str(e),
            )
    
    @abstractmethod
    def build_analysis_prompt(self, recon_context: str) -> str:
        """Build the vulnerability analysis prompt."""
        pass
    
    def parse_findings(self, response: str) -> list[Finding]:
        """Parse findings from LLM response."""
        findings = []
        
        # Try to extract JSON findings
        import re
        json_match = re.search(r'```json\s*([\s\S]*?)\s*```', response)
        if json_match:
            try:
                data = json.loads(json_match.group(1))
                if isinstance(data, list):
                    for item in data:
                        findings.append(self._dict_to_finding(item))
                elif isinstance(data, dict) and "vulnerabilities" in data:
                    for item in data["vulnerabilities"]:
                        findings.append(self._dict_to_finding(item))
            except json.JSONDecodeError:
                pass
        
        return findings
    
    def _dict_to_finding(self, data: dict) -> Finding:
        """Convert dict to Finding object."""
        return Finding(
            id=data.get("id", f"{self.vuln_type}_{len(data)}"),
            title=data.get("title", "Unknown vulnerability"),
            type=self.vuln_type,
            severity=data.get("severity", "medium"),
            endpoint=data.get("endpoint"),
            method=data.get("method"),
            parameter=data.get("parameter"),
            file_path=data.get("file_path"),
            line_number=data.get("line_number"),
            description=data.get("description", ""),
            impact=data.get("impact", ""),
            remediation=data.get("remediation", ""),
            payload=data.get("payload"),
            confidence=data.get("confidence", 0.5),
        )


class ExploitAgent(BaseAgent):
    """Base class for exploitation agents."""
    
    vuln_type: str = "unknown"
    
    def __init__(self, vulnerability_queue: list[dict] = None, **kwargs):
        super().__init__(**kwargs)
        self.vulnerability_queue = vulnerability_queue or []
    
    async def run(self) -> AgentResult:
        """Run exploitation against vulnerability queue."""
        self.start_time = time.time()
        exploited_findings = []
        
        try:
            for vuln in self.vulnerability_queue:
                logger.info(f"Attempting exploit: {vuln.get('title', 'Unknown')}")
                
                # Build exploit prompt
                prompt = self.build_exploit_prompt(vuln)
                
                # Query LLM for exploit strategy
                response = await self.query_llm(prompt)
                
                # Execute exploit
                result = await self.execute_exploit(vuln, response)
                
                if result.get("exploited"):
                    vuln["exploited"] = True
                    vuln["exploit_proof"] = result.get("proof")
                    exploited_findings.append(vuln)
            
            # Save evidence
            evidence_name = f"{self.vuln_type}_evidence.md"
            self.write_evidence(evidence_name, exploited_findings)
            
            duration_ms = int((time.time() - self.start_time) * 1000)
            
            return AgentResult(
                success=True,
                turns=self.turns,
                tokens_used=self.tokens_used,
                duration_ms=duration_ms,
                findings=exploited_findings,
                deliverable_path=str(self.deliverables_dir / evidence_name),
            )
            
        except Exception as e:
            logger.error(f"{self.agent_name} failed: {e}")
            return AgentResult(
                success=False,
                turns=self.turns,
                error=str(e),
            )
    
    @abstractmethod
    def build_exploit_prompt(self, vuln: dict) -> str:
        """Build the exploitation prompt for a vulnerability."""
        pass
    
    async def execute_exploit(self, vuln: dict, strategy: str) -> dict:
        """Execute an exploit based on LLM strategy."""
        # Default implementation - can be overridden for browser-based exploits
        return {
            "exploited": False,
            "proof": None,
            "error": "Exploitation not implemented",
        }
    
    def write_evidence(self, name: str, findings: list[dict]):
        """Write exploitation evidence."""
        path = self.deliverables_dir / name
        
        content = f"""# {self.vuln_type.upper()} Exploitation Evidence

Generated: {datetime.now(timezone.utc).isoformat()}

## Summary
- Total vulnerabilities tested: {len(self.vulnerability_queue)}
- Successfully exploited: {len(findings)}

## Exploited Vulnerabilities

"""
        for i, finding in enumerate(findings, 1):
            content += f"""
### {i}. {finding.get('title', 'Unknown')}

**Severity:** {finding.get('severity', 'Unknown')}  
**Endpoint:** {finding.get('endpoint', 'N/A')}  
**Parameter:** {finding.get('parameter', 'N/A')}

**Payload:**
```
{finding.get('payload', 'N/A')}
```

**Proof of Exploitation:**
```
{finding.get('exploit_proof', 'N/A')}
```

---
"""
        path.write_text(content)
